{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Gathering Market Capitalization Data**\n",
    "**1. Scraping Market Capitalization Data from Messari.io:** We have already scraped the market capitalization data for various cryptocurrencies from Messari.io. This data was initially stored in text files for each cryptocurrency.\n",
    "\n",
    "**2. Combining Data from Text Files:** In this notebook, we gather the market capitalization data from the individual text files. We read the data from each text file and combine it into a single DataFrame.\n",
    "\n",
    "**3. Saving Combined Data to CSV:** After combining the data into a single DataFrame, we save the combined market capitalization data to a CSV file named `market_cap_data.csv`. This CSV file will serve as a consolidated source of market capitalization data for further analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "from datetime import datetime, timedelta\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed hourly market cap data for 1inchusdt.txt .\n",
      "Processed hourly market cap data for balusdt.txt .\n",
      "Processed hourly market cap data for batusdt.txt .\n",
      "Processed hourly market cap data for crvusdt.txt .\n",
      "Processed hourly market cap data for enjusdt.txt .\n",
      "Processed hourly market cap data for ensusdt.txt .\n",
      "Processed hourly market cap data for kncusdt.txt .\n",
      "Processed hourly market cap data for linkusdt.txt .\n",
      "Processed hourly market cap data for manausdt.txt .\n",
      "Processed hourly market cap data for mkrusdt.txt .\n",
      "Processed hourly market cap data for renusdt.txt .\n",
      "Processed hourly market cap data for snxusdt.txt .\n",
      "Processed hourly market cap data for uniusdt.txt .\n",
      "Processed hourly market cap data for wbtcusdt.txt .\n",
      "Processed hourly market cap data for yfiusdt.txt .\n",
      "Processed hourly market cap data for zrxusdt.txt .\n"
     ]
    }
   ],
   "source": [
    "# Directory containing all token data files\n",
    "data_dir = 'market caps'\n",
    "\n",
    "# Initialize an empty list to store data from all tokens\n",
    "all_tokens_data = []\n",
    "\n",
    "# Process each file in the directory\n",
    "for file_name in os.listdir(data_dir):\n",
    "    if file_name.endswith('.txt'):\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        \n",
    "        with open(file_path, 'r') as file:\n",
    "            data = json.load(file)\n",
    "\n",
    "        # Extract the token name and series data\n",
    "        token_name = file_name\n",
    "        data_points = data['data']['series'][0]['points']\n",
    "\n",
    "        # Convert the data into a DataFrame\n",
    "        data_df = pd.DataFrame(data_points, columns=['timestamp', 'market_cap'])\n",
    "        data_df['token'] = token_name\n",
    "\n",
    "        # Step 1: Get market caps and timestamps into a DataFrame (already done above)\n",
    "\n",
    "        # Step 2: Remove rows with zeros and NAs\n",
    "        data_df = data_df[(data_df['market_cap'] != 0) & (~data_df['market_cap'].isna())]\n",
    "\n",
    "        # Step 3: Populate hourly data\n",
    "        data_df['timestamp'] = pd.to_datetime(data_df['timestamp'], unit='s')\n",
    "        final_data = []\n",
    "        for i in range(len(data_df) - 1):\n",
    "            current_row = data_df.iloc[i]\n",
    "            next_row = data_df.iloc[i + 1]\n",
    "\n",
    "            current_time = current_row['timestamp']\n",
    "            next_time = next_row['timestamp']\n",
    "            market_cap = next_row['market_cap']  # Use next week's market cap\n",
    "\n",
    "            # Generate hourly timestamps between current_time and next_time\n",
    "            while current_time < next_time:\n",
    "                final_data.append({'timestamp': current_time, 'market_cap': market_cap, 'token': current_row['token']})\n",
    "                current_time += timedelta(hours=1)\n",
    "\n",
    "        # Add the last timestamp of the last data point\n",
    "        final_data.append({'timestamp': data_df.iloc[-1]['timestamp'], 'market_cap': data_df.iloc[-1]['market_cap'], 'token': data_df.iloc[-1]['token']})\n",
    "\n",
    "        # Convert to DataFrame\n",
    "        hourly_df = pd.DataFrame(final_data)\n",
    "\n",
    "        # Append to all tokens data\n",
    "        all_tokens_data.append(hourly_df)\n",
    "\n",
    "        print(f\"Processed hourly market cap data for {token_name} .\")\n",
    "\n",
    "# Combine all tokens data into a single DataFrame\n",
    "combined_df = pd.concat(all_tokens_data, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def token_clean(name):\n",
    "     name = name.replace('.txt', '')\n",
    "     name = name.upper()\n",
    "     return name\n",
    "\n",
    "combined_df['token'] = combined_df['token'].apply(token_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1INCHUSDT\n",
      "34777\n",
      "2020-12-21 00:00:00\n",
      "2024-12-09 00:00:00\n",
      "BALUSDT\n",
      "39145\n",
      "2020-06-22 00:00:00\n",
      "2024-12-09 00:00:00\n",
      "BATUSDT\n",
      "43681\n",
      "2019-12-16 00:00:00\n",
      "2024-12-09 00:00:00\n",
      "CRVUSDT\n",
      "37969\n",
      "2020-08-10 00:00:00\n",
      "2024-12-09 00:00:00\n",
      "ENJUSDT\n",
      "43681\n",
      "2019-12-16 00:00:00\n",
      "2024-12-09 00:00:00\n",
      "ENSUSDT\n",
      "27049\n",
      "2021-11-08 00:00:00\n",
      "2024-12-09 00:00:00\n",
      "KNCUSDT\n",
      "29569\n",
      "2021-07-26 00:00:00\n",
      "2024-12-09 00:00:00\n",
      "LINKUSDT\n",
      "43681\n",
      "2019-12-16 00:00:00\n",
      "2024-12-09 00:00:00\n",
      "MANAUSDT\n",
      "43681\n",
      "2019-12-16 00:00:00\n",
      "2024-12-09 00:00:00\n",
      "MKRUSDT\n",
      "43681\n",
      "2019-12-16 00:00:00\n",
      "2024-12-09 00:00:00\n",
      "RENUSDT\n",
      "43681\n",
      "2019-12-16 00:00:00\n",
      "2024-12-09 00:00:00\n",
      "SNXUSDT\n",
      "43681\n",
      "2019-12-16 00:00:00\n",
      "2024-12-09 00:00:00\n",
      "UNIUSDT\n",
      "37129\n",
      "2020-09-14 00:00:00\n",
      "2024-12-09 00:00:00\n",
      "WBTCUSDT\n",
      "37465\n",
      "2020-08-31 00:00:00\n",
      "2024-12-09 00:00:00\n",
      "YFIUSDT\n",
      "38641\n",
      "2020-07-13 00:00:00\n",
      "2024-12-09 00:00:00\n",
      "ZRXUSDT\n",
      "43681\n",
      "2019-12-16 00:00:00\n",
      "2024-12-09 00:00:00\n"
     ]
    }
   ],
   "source": [
    "# ens, knc, uni, yfi\n",
    "for t in combined_df['token'].unique():\n",
    "     print(t)\n",
    "     print(len(combined_df[combined_df['token'] == t]))\n",
    "     print(min(combined_df[combined_df['token'] == t]['timestamp']))\n",
    "     print(max(combined_df[combined_df['token'] == t]['timestamp']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_df.to_csv('market_cap_data.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### End of Notebook"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
