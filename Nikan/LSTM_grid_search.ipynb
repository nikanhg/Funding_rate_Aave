{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import pandas as pd\n",
    "from mysql.connector import Error\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "import seaborn as sns\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Masking, Reshape, Layer, Lambda, Concatenate, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as be\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "from keras.layers import BatchNormalization\n",
    "from keras.regularizers import l2\n",
    "import matplotlib.pyplot as plt\n",
    "import LSTM_preprocessing as pre\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MySQL database, MySQL Server version:  8.0.39\n",
      "MySQL connection is closed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lending_rate</th>\n",
       "      <th>borrowing_rate</th>\n",
       "      <th>utilization_rate</th>\n",
       "      <th>stable_borrow_rate</th>\n",
       "      <th>crypto_symbol</th>\n",
       "      <th>date</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>coin_supply</th>\n",
       "      <th>yield</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-50.000000</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>BATUSDT</td>\n",
       "      <td>2020-12-02 14:00:00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1294545</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.008552840317602313182744400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-50.000000</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>BATUSDT</td>\n",
       "      <td>2020-12-02 15:00:00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>936344</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.008552840317602313182744400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-50.000000</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>0.013598</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>BATUSDT</td>\n",
       "      <td>2020-12-02 16:00:00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>724626</td>\n",
       "      <td>None</td>\n",
       "      <td>None</td>\n",
       "      <td>0.008552840317602313182744400</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lending_rate borrowing_rate utilization_rate stable_borrow_rate  \\\n",
       "0   -50.000000     -50.000000       -50.000000           0.030000   \n",
       "1   -50.000000     -50.000000       -50.000000           0.030000   \n",
       "2   -50.000000     -50.000000         0.013598           0.030000   \n",
       "\n",
       "  crypto_symbol                date  high   low close adj_close   volume  \\\n",
       "0       BATUSDT 2020-12-02 14:00:00  0.24  0.24  0.24      0.24  1294545   \n",
       "1       BATUSDT 2020-12-02 15:00:00  0.24  0.24  0.24      0.24   936344   \n",
       "2       BATUSDT 2020-12-02 16:00:00  0.24  0.24  0.24      0.24   724626   \n",
       "\n",
       "  market_cap coin_supply                          yield  \n",
       "0       None        None  0.008552840317602313182744400  \n",
       "1       None        None  0.008552840317602313182744400  \n",
       "2       None        None  0.008552840317602313182744400  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "connection = pre.connect_to_database()\n",
    "if connection:\n",
    "    # Query merged data\n",
    "    merged_df = pre.query_merged_crypto_data(connection)\n",
    "    # converting US bond yield from hourly to yearly under continous compounding assumptions\n",
    "    merged_df['yield'] = np.exp(merged_df['yield']*365*24)-1\n",
    "\n",
    "    # Close the connection\n",
    "    pre.query_quit(connection)\n",
    "\n",
    "merged_df.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### merging with market cap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>date</th>\n",
       "      <th>market_cap</th>\n",
       "      <th>crypto_symbol</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2020-12-21 00:00:00</td>\n",
       "      <td>87121414</td>\n",
       "      <td>1INCHUSDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>2020-12-21 01:00:00</td>\n",
       "      <td>87121414</td>\n",
       "      <td>1INCHUSDT</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>2020-12-21 02:00:00</td>\n",
       "      <td>87121414</td>\n",
       "      <td>1INCHUSDT</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                 date  market_cap crypto_symbol\n",
       "0           0  2020-12-21 00:00:00    87121414     1INCHUSDT\n",
       "1           1  2020-12-21 01:00:00    87121414     1INCHUSDT\n",
       "2           2  2020-12-21 02:00:00    87121414     1INCHUSDT"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "market_caps = pd.read_csv('market_cap_data.csv')\n",
    "market_caps = market_caps.rename(columns={\n",
    "    'timestamp': 'date',\n",
    "    'token': 'crypto_symbol',\n",
    "})\n",
    "market_caps.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lending_rate</th>\n",
       "      <th>borrowing_rate</th>\n",
       "      <th>utilization_rate</th>\n",
       "      <th>stable_borrow_rate</th>\n",
       "      <th>crypto_symbol</th>\n",
       "      <th>date</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adj_close</th>\n",
       "      <th>volume</th>\n",
       "      <th>coin_supply</th>\n",
       "      <th>yield</th>\n",
       "      <th>market_cap</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-50.000000</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>BATUSDT</td>\n",
       "      <td>2020-12-02 14:00:00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>1294545</td>\n",
       "      <td>None</td>\n",
       "      <td>0.008552840317602313182744400</td>\n",
       "      <td>317700843.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-50.000000</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>BATUSDT</td>\n",
       "      <td>2020-12-02 15:00:00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>936344</td>\n",
       "      <td>None</td>\n",
       "      <td>0.008552840317602313182744400</td>\n",
       "      <td>317700843.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-50.000000</td>\n",
       "      <td>-50.000000</td>\n",
       "      <td>0.013598</td>\n",
       "      <td>0.030000</td>\n",
       "      <td>BATUSDT</td>\n",
       "      <td>2020-12-02 16:00:00</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>0.24</td>\n",
       "      <td>724626</td>\n",
       "      <td>None</td>\n",
       "      <td>0.008552840317602313182744400</td>\n",
       "      <td>317700843.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  lending_rate borrowing_rate utilization_rate stable_borrow_rate  \\\n",
       "0   -50.000000     -50.000000       -50.000000           0.030000   \n",
       "1   -50.000000     -50.000000       -50.000000           0.030000   \n",
       "2   -50.000000     -50.000000         0.013598           0.030000   \n",
       "\n",
       "  crypto_symbol                date  high   low close adj_close   volume  \\\n",
       "0       BATUSDT 2020-12-02 14:00:00  0.24  0.24  0.24      0.24  1294545   \n",
       "1       BATUSDT 2020-12-02 15:00:00  0.24  0.24  0.24      0.24   936344   \n",
       "2       BATUSDT 2020-12-02 16:00:00  0.24  0.24  0.24      0.24   724626   \n",
       "\n",
       "  coin_supply                          yield   market_cap  \n",
       "0        None  0.008552840317602313182744400  317700843.0  \n",
       "1        None  0.008552840317602313182744400  317700843.0  \n",
       "2        None  0.008552840317602313182744400  317700843.0  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Convert 'date' columns to pandas datetime format\n",
    "merged_df['date'] = pd.to_datetime(merged_df['date'])\n",
    "market_caps['date'] = pd.to_datetime(market_caps['date'])\n",
    "\n",
    "# Perform the merge based on 'crypto_symbol' and 'date', keeping only rows in merged_df\n",
    "merged_df.drop('market_cap', axis = 1, inplace=True)\n",
    "merged_df = pd.merge(\n",
    "    merged_df,\n",
    "    market_caps[['date', 'market_cap', 'crypto_symbol']],\n",
    "    on=['date', 'crypto_symbol'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "merged_df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0fb17d947aa443e8982dc7816c1649a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       ".gitattributes:   0%|          | 0.00/1.23k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1be70694b58c49cda6ee20f6b62a0350",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "1_Pooling/config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "15f52a7423ed4e97a49f1ba2acb8951b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/10.7k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "359e6ce455f14992a39e42c42773faa2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/612 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dcbdc9855b014b1fba88e2952193e258",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6290ded478a94091b51d02d36a242238",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data_config.json:   0%|          | 0.00/39.3k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b16cc58c5cab4d7c98c79f54ac571194",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3ae27faf52624e83bf010356db90fd70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12cef23dfaba4a4a8bdce36bbe691502",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O1.onnx:   0%|          | 0.00/90.4M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "40a0cce577be41e681db6611d01171cf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O2.onnx:   0%|          | 0.00/90.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c241c7ea400443ff973a375ec55aa664",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O3.onnx:   0%|          | 0.00/90.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fc9d071befc84ea2aff63faeaa01faa9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_O4.onnx:   0%|          | 0.00/45.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0c3f5a6a35ce46338f8dcb82a5f9cdea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_qint8_arm64.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c5cd352073764d78b88023dff13fdaf5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_qint8_avx512.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e416ded3b99f426c94ca85eddfb17cf9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_qint8_avx512_vnni.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3c3adf565df4f3ab354c91cc4e5b2a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model_quint8_avx2.onnx:   0%|          | 0.00/23.0M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4210f5b2f3ba4761b47d65721493ab91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_model.bin:   0%|          | 0.00/90.3M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d57e05e2b204405d9a1d178fd5c83922",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino/openvino_model.xml:   0%|          | 0.00/211k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9c25b8ad11c499ea76358aa5763dda6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "openvino_model_qint8_quantized.bin:   0%|          | 0.00/22.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "67ba5f37d5f846de8b4eb910fccfa201",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "(…)nvino/openvino_model_qint8_quantized.xml:   0%|          | 0.00/368k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4467a2cea7ea4cc08fc8dc9786818100",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model.bin:   0%|          | 0.00/90.9M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "746715e622a54c4e802061c0bfce42ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6c5e681ef63e4d8d974e5ad924b569a5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/112 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a671c7f922274dfcbcb52dcc90fcd123",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "acd29ab8491c4c8c8253dd7e1ed938e4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/350 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "763053ca88c44aeb8fbd9bbd953dd5bf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train_script.py:   0%|          | 0.00/13.2k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cc6caf24de9e47b98c6b3df247e2e54c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6a81b87318a649b89718e72abc753d3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "merged_df_emb = pre.create_llm_embeddings(merged_df, \"crypto_symbol\", n_components=15)\n",
    "merged_df_emb = pre.create_cyclical_encodings(merged_df_emb, \"date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['lending_rate', 'borrowing_rate', 'utilization_rate',\n",
       "       'stable_borrow_rate', 'crypto_symbol', 'date', 'high', 'low', 'close',\n",
       "       'adj_close', 'volume', 'coin_supply', 'yield', 'market_cap',\n",
       "       'crypto_symbol_embedding_1', 'crypto_symbol_embedding_2',\n",
       "       'crypto_symbol_embedding_3', 'crypto_symbol_embedding_4',\n",
       "       'crypto_symbol_embedding_5', 'crypto_symbol_embedding_6',\n",
       "       'crypto_symbol_embedding_7', 'crypto_symbol_embedding_8',\n",
       "       'crypto_symbol_embedding_9', 'crypto_symbol_embedding_10',\n",
       "       'crypto_symbol_embedding_11', 'crypto_symbol_embedding_12',\n",
       "       'crypto_symbol_embedding_13', 'crypto_symbol_embedding_14',\n",
       "       'crypto_symbol_embedding_15', 'Month_Sine', 'Month_Cosine', 'Day_Sine',\n",
       "       'Day_Cosine', 'Hour_Sine', 'Hour_Cosine', 'DayofWeek_Sine',\n",
       "       'DayofWeek_Cosine'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df_emb.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "418650\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Index(['lending_rate', 'borrowing_rate', 'utilization_rate', 'crypto_symbol',\n",
       "       'date', 'high', 'low', 'close', 'volume', 'yield', 'market_cap',\n",
       "       'crypto_symbol_embedding_1', 'crypto_symbol_embedding_2',\n",
       "       'crypto_symbol_embedding_3', 'crypto_symbol_embedding_4',\n",
       "       'crypto_symbol_embedding_5', 'crypto_symbol_embedding_6',\n",
       "       'crypto_symbol_embedding_7', 'crypto_symbol_embedding_8',\n",
       "       'crypto_symbol_embedding_9', 'crypto_symbol_embedding_10',\n",
       "       'crypto_symbol_embedding_11', 'crypto_symbol_embedding_12',\n",
       "       'crypto_symbol_embedding_13', 'crypto_symbol_embedding_14',\n",
       "       'crypto_symbol_embedding_15', 'Month_Sine', 'Month_Cosine', 'Day_Sine',\n",
       "       'Day_Cosine', 'Hour_Sine', 'Hour_Cosine', 'DayofWeek_Sine',\n",
       "       'DayofWeek_Cosine', 'attention'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data without NA rows if we needed \n",
    "filtered_df = merged_df_emb[(merged_df['borrowing_rate'] != -50)&(merged_df['lending_rate'] != -50)&(merged_df['utilization_rate'] != -50)]\n",
    "filtered_df['attention'] = filtered_df['volume'] * filtered_df['close']\n",
    "filtered_df.drop(columns=['stable_borrow_rate','adj_close','coin_supply'], inplace=True)\n",
    "filtered_df[['lending_rate','borrowing_rate','utilization_rate','close', 'volume']] = filtered_df[['lending_rate','borrowing_rate','utilization_rate','close', 'volume']].astype(float)\n",
    "filtered_df.reset_index(inplace=True, drop=True)\n",
    "print(len(filtered_df))\n",
    "filtered_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We scale  market cap, attention, yield together as they are comparable\n",
    "scaler = MinMaxScaler(feature_range=(0, 1))  # Default range is (0, 1)\n",
    "filtered_df['yield'] = scaler.fit_transform(filtered_df[['yield']])\n",
    "filtered_df['market_cap'] = scaler.fit_transform(filtered_df[['market_cap']])\n",
    "filtered_df['attention'] = scaler.fit_transform(filtered_df[['attention']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "outlier_threshold = 1.5\n",
    "input_window = 48\n",
    "output_window = 24\n",
    "input_columns = ['lending_rate', 'borrowing_rate', 'utilization_rate',\n",
    "                 'close', 'volume', 'yield', 'market_cap',\n",
    "       'crypto_symbol_embedding_1', 'crypto_symbol_embedding_2',\n",
    "       'crypto_symbol_embedding_3', 'crypto_symbol_embedding_4',\n",
    "       'crypto_symbol_embedding_5', 'crypto_symbol_embedding_6',\n",
    "       'crypto_symbol_embedding_7', 'crypto_symbol_embedding_8',\n",
    "       'crypto_symbol_embedding_9', 'crypto_symbol_embedding_10',\n",
    "       'crypto_symbol_embedding_11', 'crypto_symbol_embedding_12',\n",
    "       'crypto_symbol_embedding_13', 'crypto_symbol_embedding_14',\n",
    "       'crypto_symbol_embedding_15', 'Month_Sine', 'Month_Cosine', 'Day_Sine',\n",
    "       'Day_Cosine', 'Hour_Sine', 'Hour_Cosine', 'DayofWeek_Sine',\n",
    "       'DayofWeek_Cosine', 'attention','returns']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATUSDT 28159\n",
      "rows with valid returns: 27850\n",
      "input size (10222, 48, 32)\n",
      "output size (10222, 2)\n",
      "LINKUSDT 31934\n",
      "rows with valid returns: 31598\n",
      "input size (8765, 48, 32)\n",
      "output size (8765, 2)\n",
      "KNCUSDT 27690\n",
      "rows with valid returns: 27474\n",
      "input size (5137, 48, 32)\n",
      "output size (5137, 2)\n",
      "MKRUSDT 30094\n",
      "rows with valid returns: 29657\n",
      "input size (7645, 48, 32)\n",
      "output size (7645, 2)\n",
      "MANAUSDT 27145\n",
      "rows with valid returns: 26560\n",
      "input size (2858, 48, 32)\n",
      "output size (2858, 2)\n",
      "ZRXUSDT 28255\n",
      "rows with valid returns: 28057\n",
      "input size (11747, 48, 32)\n",
      "output size (11747, 2)\n",
      "SNXUSDT 31087\n",
      "rows with valid returns: 30477\n",
      "input size (4984, 48, 32)\n",
      "output size (4984, 2)\n",
      "WBTCUSDT 11729\n",
      "rows with valid returns: 11297\n",
      "input size (244, 48, 32)\n",
      "output size (244, 2)\n",
      "ENJUSDT 27034\n",
      "rows with valid returns: 26606\n",
      "input size (7331, 48, 32)\n",
      "output size (7331, 2)\n",
      "RENUSDT 27658\n",
      "rows with valid returns: 27296\n",
      "input size (7834, 48, 32)\n",
      "output size (7834, 2)\n",
      "YFIUSDT 29900\n",
      "rows with valid returns: 29426\n",
      "input size (2856, 48, 32)\n",
      "output size (2856, 2)\n",
      "UNIUSDT 29885\n",
      "rows with valid returns: 29300\n",
      "input size (3954, 48, 32)\n",
      "output size (3954, 2)\n",
      "CRVUSDT 28455\n",
      "rows with valid returns: 27458\n",
      "input size (1004, 48, 32)\n",
      "output size (1004, 2)\n",
      "BALUSDT 24855\n",
      "rows with valid returns: 24317\n",
      "input size (1741, 48, 32)\n",
      "output size (1741, 2)\n",
      "ENSUSDT 20745\n",
      "rows with valid returns: 20574\n",
      "input size (6319, 48, 32)\n",
      "output size (6319, 2)\n",
      "1INCHUSDT 14025\n",
      "rows with valid returns: 13930\n",
      "input size (4217, 48, 32)\n",
      "output size (4217, 2)\n"
     ]
    }
   ],
   "source": [
    "# Initialize empty lists to store aggregated inputs and targets\n",
    "all_inputs = []\n",
    "all_targets = []\n",
    "\n",
    "# looping through symbols\n",
    "symbols = filtered_df['crypto_symbol'].unique()\n",
    "for s in symbols:\n",
    "     try:\n",
    "          sim_df = filtered_df[filtered_df['crypto_symbol'] == s]\n",
    "          sim_df.reset_index(inplace=True, drop=True)\n",
    "          print(s, len(sim_df))\n",
    "          # First Loop: Calculate intervals for each column without modifying the DataFrame\n",
    "          intervals = {}\n",
    "          for column in ['lending_rate', 'borrowing_rate', 'utilization_rate']:\n",
    "               lower, upper = pre.calculate_iqr_bounds(sim_df[column],outlier_threshold)\n",
    "               intervals[column] = {'lower_bound': lower, 'upper_bound': upper}\n",
    "\n",
    "          # getting the returns\n",
    "          reduced_df = sim_df.copy()\n",
    "          reduced_df['returns'] = pre.calculate_hourly_returns(reduced_df, 'date', 'close')\n",
    "          reduced_df = reduced_df[reduced_df['returns'].notna()]\n",
    "          print('rows with valid returns:', len(reduced_df))\n",
    "          reduced_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "          # Second Loop: Filter rows based on the pre-calculated intervals\n",
    "          for column in ['lending_rate', 'borrowing_rate', 'utilization_rate']:\n",
    "               lower_bound = intervals[column]['lower_bound']\n",
    "               upper_bound = intervals[column]['upper_bound']\n",
    "               # Apply filtering based on pre-calculated bounds\n",
    "               reduced_df = reduced_df[(reduced_df[column] > lower_bound) & (reduced_df[column] < upper_bound)]\n",
    "\n",
    "          reduced_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "          # MinMax scaling\n",
    "          scaler = MinMaxScaler(feature_range=(0, 1))  # Default range is (0, 1)\n",
    "\n",
    "          scaled_df = reduced_df.copy()\n",
    "          scaled_df['lending_rate'] = scaler.fit_transform(reduced_df[['lending_rate']])\n",
    "          scaled_df['borrowing_rate'] = scaler.fit_transform(reduced_df[['borrowing_rate']])\n",
    "          scaled_df['utilization_rate'] = scaler.fit_transform(reduced_df[['utilization_rate']])\n",
    "          scaled_df['close'] = scaler.fit_transform(reduced_df[['close']])\n",
    "          scaled_df['volume'] = scaler.fit_transform(reduced_df[['volume']])\n",
    "          scaled_df['returns'] = scaler.fit_transform(reduced_df[['returns']])\n",
    "\n",
    "          inputs, targets = pre.extract_valid_windows_v2(\n",
    "               scaled_df,\n",
    "               'date', \n",
    "               input_window, output_window, \n",
    "               input_columns, \n",
    "               ['lending_rate','borrowing_rate']\n",
    "               )\n",
    "\n",
    "          # Append results from the current DataFrame\n",
    "          all_inputs.append(inputs)\n",
    "          all_targets.append(targets)\n",
    "\n",
    "          print('input size', inputs.shape)\n",
    "          print('output size', targets.shape)\n",
    "\n",
    "     except Exception as e:\n",
    "        # Handle any other exceptions\n",
    "        print(f\"Unexpected error in symbol {s}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total input size (86858, 48, 32)\n",
      "total output size (86858, 2)\n"
     ]
    }
   ],
   "source": [
    "# Concatenate all inputs and targets into single arrays\n",
    "all_inputs = np.concatenate(all_inputs, axis=0) if all_inputs else np.array([])\n",
    "all_targets = np.concatenate(all_targets, axis=0) if all_targets else np.array([])\n",
    "print('total input size', all_inputs.shape)\n",
    "print('total output size', all_targets.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86858, 1)\n",
      "(86858, 1)\n"
     ]
    }
   ],
   "source": [
    "# Lending rate\n",
    "output_1 = all_targets[:, 0].reshape(all_targets.shape[0], 1)  # First column reshaped to (86858, 1)\n",
    "# Borrowing rate\n",
    "output_2 = all_targets[:, 1].reshape(all_targets.shape[0], 1)  # Second column reshaped to (86858, 1)\n",
    "print(output_1.shape)  # Should print (86858, 1)\n",
    "print(output_2.shape)  # Should print (86858, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y_predict, y_test = pre.train_v1(all_inputs, output_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 48, 32)]          0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (None, 48, 80)            36160     \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 48, 80)            320       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 48, 80)            0         \n",
      "                                                                 \n",
      " lstm_1 (LSTM)               (None, 40)                19360     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 40)                160       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 40)                0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 3)                 123       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 56123 (219.23 KB)\n",
      "Trainable params: 55883 (218.29 KB)\n",
      "Non-trainable params: 240 (960.00 Byte)\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "522/522 [==============================] - 36s 62ms/step - loss: nan - accuracy: 0.5764 - val_loss: nan - val_accuracy: 0.5810\n",
      "Epoch 2/10\n",
      "522/522 [==============================] - 33s 63ms/step - loss: nan - accuracy: 0.5772 - val_loss: nan - val_accuracy: 0.5810\n",
      "Epoch 3/10\n",
      "522/522 [==============================] - 33s 63ms/step - loss: nan - accuracy: 0.5772 - val_loss: nan - val_accuracy: 0.5810\n",
      "Epoch 4/10\n",
      "522/522 [==============================] - 32s 62ms/step - loss: nan - accuracy: 0.5772 - val_loss: nan - val_accuracy: 0.5810\n",
      "Epoch 5/10\n",
      "522/522 [==============================] - 33s 63ms/step - loss: nan - accuracy: 0.5772 - val_loss: nan - val_accuracy: 0.5810\n",
      "Epoch 6/10\n",
      "522/522 [==============================] - 31s 60ms/step - loss: nan - accuracy: 0.5772 - val_loss: nan - val_accuracy: 0.5810\n",
      "Epoch 7/10\n",
      "522/522 [==============================] - 32s 60ms/step - loss: nan - accuracy: 0.5772 - val_loss: nan - val_accuracy: 0.5810\n",
      "Epoch 8/10\n",
      "522/522 [==============================] - 32s 62ms/step - loss: nan - accuracy: 0.5772 - val_loss: nan - val_accuracy: 0.5810\n",
      "Epoch 9/10\n",
      "522/522 [==============================] - 33s 63ms/step - loss: nan - accuracy: 0.5772 - val_loss: nan - val_accuracy: 0.5810\n",
      "Epoch 10/10\n",
      "522/522 [==============================] - 33s 63ms/step - loss: nan - accuracy: 0.5772 - val_loss: nan - val_accuracy: 0.5810\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'class_loss'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[24], line 86\u001b[0m\n\u001b[0;32m     82\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y_pred, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     84\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m y_test, y_pred\n\u001b[1;32m---> 86\u001b[0m y_test, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_v1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_1\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[24], line 71\u001b[0m, in \u001b[0;36mtrain_v1\u001b[1;34m(inputs, outputs, test_size, valid_size, epochs, batch_size, d1, d2, cell_size)\u001b[0m\n\u001b[0;32m     68\u001b[0m class_val_loss_key \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mval_class_loss\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# Plot the class_predictions losses\u001b[39;00m\n\u001b[1;32m---> 71\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(\u001b[43mhistory\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhistory\u001b[49m\u001b[43m[\u001b[49m\u001b[43mclass_train_loss_key\u001b[49m\u001b[43m]\u001b[49m, label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass Train Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     72\u001b[0m plt\u001b[38;5;241m.\u001b[39mplot(history\u001b[38;5;241m.\u001b[39mhistory[class_val_loss_key], label\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mClass Validation Loss\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# Add labels and legend\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'class_loss'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Masking, Reshape, Layer, Lambda, Concatenate, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "def train_v1(inputs, outputs, test_size=0.2, valid_size=0.25, epochs=5, batch_size=100, d1=0.1, d2 = 0.05, cell_size = 80):\n",
    "    # Clearing the TensorFlow session to ensure the model starts with fresh weights and biases\n",
    "    tf.keras.backend.clear_session()\n",
    "    n_classes = 3\n",
    "\n",
    "    cell_size_1 = cell_size\n",
    "    cell_size_2 = cell_size_1//2\n",
    "\n",
    "    # Splitting the data into train+validation and test sets\n",
    "    X_train_valid, X_test, y_train_valid, y_test = train_test_split(\n",
    "        inputs, outputs, test_size=test_size)\n",
    "\n",
    "    # Splitting the train+validation set into separate training and validation sets\n",
    "    X_train, X_valid, y_train, y_valid = train_test_split(\n",
    "        X_train_valid, y_train_valid, test_size=valid_size)\n",
    "\n",
    "    # Model definition\n",
    "    inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    Lstm_layer_1 = LSTM(cell_size_1, return_sequences=True, stateful=False)(inputs)\n",
    "    Batch_norm_1 = BatchNormalization()(Lstm_layer_1)\n",
    "    Dropout_layer_1 = Dropout(d1)(Batch_norm_1)\n",
    "    Lstm_layer_2 = LSTM(cell_size_2, return_sequences=False, stateful=False)(Dropout_layer_1)  # just halved\n",
    "    Batch_norm_2 = BatchNormalization()(Lstm_layer_2)\n",
    "    Drouput_layer_2 = Dropout(d2)(Batch_norm_2)\n",
    "    predictions = Dense(n_classes, activation='softmax')(Drouput_layer_2)\n",
    "    LSTM_base = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "    LSTM_base.summary()\n",
    "    # optimizer\n",
    "    optimizer = Adadelta(\n",
    "    learning_rate=1.0,\n",
    "    rho=0.8,\n",
    "    epsilon=1e-7)      # Default , to prevent division by zero)\n",
    "\n",
    "\n",
    "    # Compiling the model\n",
    "    def sparse_crossentropy_masked(y_true, y_pred):\n",
    "        y_true = tf.cast(y_true, tf.int32)\n",
    "        y_pred = tf.cast(y_pred, tf.float32)\n",
    "        loss = tf.compat.v1.losses.sparse_softmax_cross_entropy(y_true, y_pred)\n",
    "        return tf.reduce_mean(loss)\n",
    "    \n",
    "    LSTM_base.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss=sparse_crossentropy_masked,\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    # Training the model\n",
    "    history = LSTM_base.fit(x=X_train, y=y_train,\n",
    "                    validation_data=(X_valid, y_valid),\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=False)\n",
    "\n",
    "    # Extract the specific loss keys for class_predictions\n",
    "    class_train_loss_key = 'class_loss'\n",
    "    class_val_loss_key = 'val_class_loss'\n",
    "\n",
    "    # Plot the class_predictions losses\n",
    "    plt.plot(history.history[class_train_loss_key], label='Class Train Loss')\n",
    "    plt.plot(history.history[class_val_loss_key], label='Class Validation Loss')\n",
    "\n",
    "    # Add labels and legend\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.title('Training and Validation Losses Class Prediction')\n",
    "    plt.show()\n",
    "\n",
    "    y_pred = LSTM_base.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=-1)\n",
    "\n",
    "    return y_test, y_pred\n",
    "\n",
    "y_test, y_pred = train_v1(all_inputs, output_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the validity of y_pred and y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this should work as well\n",
    "print(pre.classification_metrics(y_test, y_pred, printed = False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
