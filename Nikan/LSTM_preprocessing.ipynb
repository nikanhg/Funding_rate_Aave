{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate outlier bounds using IQR\n",
    "def calculate_iqr_bounds(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = max(Q1 - 1.5 * IQR, 0)\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# calculate returns on valid windows\n",
    "def calculate_hourly_returns(df, date_col, close_col):\n",
    "    \"\"\"\n",
    "    Calculates returns based on the close price, only if the date difference is 1 hour.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the time series data.\n",
    "        date_col (str): The name of the datetime column.\n",
    "        close_col (str): The name of the close price column.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Series containing the calculated returns or None for invalid rows.\n",
    "    \"\"\"\n",
    "    # Ensure the date column is in datetime format\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    \n",
    "    # Sort by date to ensure sequential order\n",
    "    df = df.sort_values(by=date_col).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate the time difference between consecutive rows in hours\n",
    "    time_diff = df[date_col].diff().dt.total_seconds() / 3600\n",
    "    \n",
    "    # Calculate returns only for rows where time_diff == 1 hour\n",
    "    returns = np.where(\n",
    "        time_diff == 1,\n",
    "        (df[close_col] - df[close_col].shift(1)) / df[close_col].shift(1),\n",
    "        None\n",
    "    )\n",
    "    \n",
    "    return pd.Series(returns, index=df.index)\n",
    "\n",
    "\n",
    "# now we have a dataframe that does not have any NA and ay outlier, but its time series is corrupted, therefore we need valid windows\n",
    "def extract_valid_windows(df, date_col, input_window, target_window, input_columns, target_columns):\n",
    "    \"\"\"\n",
    "    Extracts valid windows from a time series DataFrame for LSTM training.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The time series DataFrame with a datetime column.\n",
    "        date_col (str): The name of the datetime column.\n",
    "        input_window (int): The number of timesteps for the input sequence.\n",
    "        target_window (int): The number of timesteps for the target sequence.\n",
    "        input_columns (list of str): List of column names to include in the input data.\n",
    "        target_columns (list of str): List of column names to include in the target data.\n",
    "        \n",
    "    Returns:\n",
    "        inputs (list of np.ndarray): List of valid input sequences.\n",
    "        targets (list of np.ndarray): List of corresponding target sequences.\n",
    "    \"\"\"\n",
    "    # Sort by the datetime column to ensure the time series is ordered\n",
    "    df = df.sort_values(by=date_col).reset_index(drop=True)\n",
    "    \n",
    "    # Ensure the datetime column is in pandas datetime format\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    \n",
    "    # Identify valid consecutive rows (1-hour apart)\n",
    "    time_diffs = df[date_col].diff().dt.total_seconds()\n",
    "    valid_indices = time_diffs == 3600  # 1 hour = 3600 seconds\n",
    "    \n",
    "    # Mark valid sequences\n",
    "    valid_sequence_flags = valid_indices | valid_indices.shift(-1, fill_value=False)\n",
    "    df = df[valid_sequence_flags].reset_index(drop=True)\n",
    "\n",
    "    # Prepare inputs and targets\n",
    "    inputs, targets = [], []\n",
    "    total_window = input_window + target_window\n",
    "\n",
    "    for i in range(len(df) - total_window + 1):\n",
    "        # Extract a potential window of size `total_window`\n",
    "        window = df.iloc[i:i+total_window]\n",
    "        \n",
    "        # Check if all rows in the window are 1-hour apart\n",
    "        if (window[date_col].diff().dt.total_seconds()[1:] == 3600).all():\n",
    "            # Split into input and target based on specified columns\n",
    "            input_data = window.iloc[:input_window][input_columns].values\n",
    "            target_data = window.iloc[input_window:][target_columns].values\n",
    "            inputs.append(input_data)\n",
    "            targets.append(target_data)\n",
    "\n",
    "    return np.array(inputs), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MySQL database, MySQL Server version:  8.0.39\n",
      "\n",
      "Merged DataFrame:\n",
      "   id crypto_symbol                date lending_rate borrowing_rate  \\\n",
      "0   1       BATUSDT 2020-12-02 14:00:00   -50.000000     -50.000000   \n",
      "1   2       BATUSDT 2020-12-02 15:00:00   -50.000000     -50.000000   \n",
      "2   3       BATUSDT 2020-12-02 16:00:00   -50.000000     -50.000000   \n",
      "3   4       BATUSDT 2020-12-02 17:00:00   -50.000000     -50.000000   \n",
      "4   5       BATUSDT 2020-12-02 18:00:00   -50.000000     -50.000000   \n",
      "\n",
      "  utilization_rate stable_borrow_rate crypto_symbol                date  high  \\\n",
      "0       -50.000000           0.030000       BATUSDT 2020-12-02 14:00:00  0.24   \n",
      "1       -50.000000           0.030000       BATUSDT 2020-12-02 15:00:00  0.24   \n",
      "2         0.013598           0.030000       BATUSDT 2020-12-02 16:00:00  0.24   \n",
      "3         0.014834           0.030000       BATUSDT 2020-12-02 17:00:00  0.24   \n",
      "4         0.014834           0.030000       BATUSDT 2020-12-02 18:00:00  0.25   \n",
      "\n",
      "    low close adj_close   volume market_cap coin_supply  \n",
      "0  0.24  0.24      0.24  1294545       None        None  \n",
      "1  0.24  0.24      0.24   936344       None        None  \n",
      "2  0.24  0.24      0.24   724626       None        None  \n",
      "3  0.24  0.24      0.24   710071       None        None  \n",
      "4  0.24  0.24      0.24  1124485       None        None  \n",
      "\n",
      "Merged data saved to 'merged_crypto_data.csv'\n",
      "MySQL connection is closed\n"
     ]
    }
   ],
   "source": [
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "import pandas as pd\n",
    "\n",
    "# MySQL database connection function\n",
    "def connect_to_database():\n",
    "    try:\n",
    "        # Establishing connection to the database\n",
    "        connection = mysql.connector.connect(\n",
    "            host='crypto-matter.c5eq66ogk1mf.eu-central-1.rds.amazonaws.com',\n",
    "            database='Crypto',\n",
    "            user='Jing',  # Replace with your actual first name\n",
    "            password='Crypto12!'\n",
    "        )\n",
    "\n",
    "        if connection.is_connected():\n",
    "            db_info = connection.get_server_info()\n",
    "            print(\"Connected to MySQL database, MySQL Server version: \", db_info)\n",
    "            return connection\n",
    "\n",
    "    except Error as e:\n",
    "        print(\"Error while connecting to MySQL\", e)\n",
    "        return None\n",
    "\n",
    "\n",
    "\n",
    "# Function to query merged data from crypto_lending_borrowing and crypto_price tables\n",
    "def query_merged_crypto_data(connection):\n",
    "    query = \"\"\"\n",
    "    SELECT clb.*, cp.*\n",
    "    FROM crypto_lending_borrowing clb\n",
    "    JOIN crypto_price cp \n",
    "        ON clb.crypto_symbol = cp.crypto_symbol\n",
    "        AND clb.date = cp.date\n",
    "    WHERE UPPER(clb.crypto_symbol) IN ('1INCHUSDT', 'BALUSDT', 'BATUSDT', 'CRVUSDT', 'ENJUSDT', 'ENSUSDT', 'KNCUSDT', 'LINKUSDT', 'MANAUSDT', 'MKRUSDT', 'RENUSDT', 'SNXUSDT', 'UNIUSDT', 'WBTCUSDT', 'YFIUSDT', 'ZRXUSDT')\n",
    "    \"\"\"\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    try:\n",
    "        # Execute the query\n",
    "        cursor.execute(query)\n",
    "\n",
    "        # Fetch all results\n",
    "        results = cursor.fetchall()\n",
    "\n",
    "        # Get column names from cursor description\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "\n",
    "        # Convert results to a Pandas DataFrame\n",
    "        df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Error as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "# Function to close the database connection\n",
    "def query_quit(connection):\n",
    "    if connection.is_connected():\n",
    "        connection.close()\n",
    "        print(\"MySQL connection is closed\")\n",
    "\n",
    "\n",
    "connection = connect_to_database()\n",
    "\n",
    "if connection:\n",
    "    # Query merged data\n",
    "    merged_df = query_merged_crypto_data(connection)\n",
    "\n",
    "    if merged_df is not None and not merged_df.empty:\n",
    "        # Display first few rows of the DataFrame\n",
    "        print(\"\\nMerged DataFrame:\")\n",
    "        print(merged_df.head())\n",
    "\n",
    "        # Save DataFrame to CSV\n",
    "        merged_df.to_csv('merged_crypto_data.csv', index=False)\n",
    "        print(\"\\nMerged data saved to 'merged_crypto_data.csv'\")\n",
    "    else:\n",
    "        print(\"\\nNo data found after merging.\")\n",
    "\n",
    "    # Close the connection\n",
    "    query_quit(connection)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\hgnik\\AppData\\Local\\Temp\\ipykernel_43104\\3944585468.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n"
     ]
    }
   ],
   "source": [
    "# Drop the second occurrence of a specific column\n",
    "merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "# data without NA rows if we needed \n",
    "filtered_df = merged_df[(merged_df['borrowing_rate'] != -50)&(merged_df['lending_rate'] != -50)&(merged_df['utilization_rate'] != -50)]\n",
    "filtered_df.reset_index(inplace=True, drop=True)\n",
    "# date formatting\n",
    "filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n",
    "# taking the columns we want and converting them to floats\n",
    "filtered_df = filtered_df[['crypto_symbol', 'date', 'lending_rate',\t'borrowing_rate','utilization_rate','close', 'volume']]\n",
    "filtered_df[['lending_rate',\t'borrowing_rate','utilization_rate','close', 'volume']] = filtered_df[['lending_rate','borrowing_rate','utilization_rate','close', 'volume']].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATUSDT 28159\n",
      "LINKUSDT 31934\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize empty lists to store aggregated inputs and targets\n",
    "all_inputs = []\n",
    "all_targets = []\n",
    "\n",
    "# looping through symbols\n",
    "symbols = filtered_df['crypto_symbol'].unique()\n",
    "for s in symbols:\n",
    "     sim_df = filtered_df[filtered_df['crypto_symbol'] == s]\n",
    "     sim_df.reset_index(inplace=True, drop=True)\n",
    "     print(s, len(sim_df))\n",
    "     # First Loop: Calculate intervals for each column without modifying the DataFrame\n",
    "     intervals = {}\n",
    "     for column in ['lending_rate', 'borrowing_rate', 'utilization_rate']:\n",
    "          lower, upper = calculate_iqr_bounds(sim_df[column])\n",
    "          intervals[column] = {'lower_bound': lower, 'upper_bound': upper}\n",
    "\n",
    "     # Second Loop: Filter rows based on the pre-calculated intervals\n",
    "     reduced_df = sim_df.copy()\n",
    "     for column in ['lending_rate', 'borrowing_rate', 'utilization_rate']:\n",
    "          lower_bound = intervals[column]['lower_bound']\n",
    "          upper_bound = intervals[column]['upper_bound']\n",
    "          # Apply filtering based on pre-calculated bounds\n",
    "          reduced_df = reduced_df[(reduced_df[column] > lower_bound) & (reduced_df[column] < upper_bound)]\n",
    "\n",
    "     reduced_df['returns'] = calculate_hourly_returns(reduced_df, 'date', 'close')\n",
    "\n",
    "     # MinMax scaling\n",
    "     scaler = MinMaxScaler(feature_range=(0, 1))  # Default range is (0, 1)\n",
    "\n",
    "     scaled_df = reduced_df.copy()\n",
    "     scaled_df['lending_rate'] = scaler.fit_transform(reduced_df[['lending_rate']])\n",
    "     scaled_df['borrowing_rate'] = scaler.fit_transform(reduced_df[['borrowing_rate']])\n",
    "     scaled_df['utilization_rate'] = scaler.fit_transform(reduced_df[['utilization_rate']])\n",
    "     scaled_df['close'] = scaler.fit_transform(reduced_df[['close']])\n",
    "     scaled_df['volume'] = scaler.fit_transform(reduced_df[['volume']])\n",
    "     scaled_df['returns'] = scaler.fit_transform(reduced_df[['returns']])\n",
    "\n",
    "     inputs, targets = extract_valid_windows(\n",
    "          scaled_df,\n",
    "          'date', \n",
    "          40, 10, \n",
    "          ['lending_rate',\t'borrowing_rate','utilization_rate','returns', 'volume'], \n",
    "          ['lending_rate','borrowing_rate']\n",
    "          )\n",
    "     \n",
    "     # Append results from the current DataFrame\n",
    "     all_inputs.append(inputs)\n",
    "     all_targets.append(targets)\n",
    "     \n",
    "\n",
    "# Concatenate all inputs and targets into single arrays\n",
    "all_inputs = np.concatenate(all_inputs, axis=0) if all_inputs else np.array([])\n",
    "all_targets = np.concatenate(all_targets, axis=0) if all_targets else np.array([])\n",
    "     \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(23556, 40, 5)\n",
      "(23556, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "print(all_inputs.shape)\n",
    "print(all_targets.shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
