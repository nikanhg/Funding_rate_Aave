{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to calculate outlier bounds using IQR\n",
    "def calculate_iqr_bounds(series):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = max(Q1 - 1.5 * IQR, 0)\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# calculate returns on valid windows\n",
    "def calculate_hourly_returns(df, date_col, close_col):\n",
    "    \"\"\"\n",
    "    Calculates returns based on the close price, only if the date difference is 1 hour.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the time series data.\n",
    "        date_col (str): The name of the datetime column.\n",
    "        close_col (str): The name of the close price column.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Series containing the calculated returns or None for invalid rows.\n",
    "    \"\"\"\n",
    "    # Ensure the date column is in datetime format\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    \n",
    "    # Sort by date to ensure sequential order\n",
    "    df = df.sort_values(by=date_col).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate the time difference between consecutive rows in hours\n",
    "    time_diff = df[date_col].diff().dt.total_seconds() / 3600\n",
    "    \n",
    "    # Calculate returns only for rows where time_diff == 1 hour\n",
    "    returns = np.where(\n",
    "        time_diff == 1,\n",
    "        (df[close_col] - df[close_col].shift(1)) / df[close_col].shift(1),\n",
    "        None\n",
    "    )\n",
    "    \n",
    "    return pd.Series(returns, index=df.index)\n",
    "\n",
    "\n",
    "# now we have a dataframe that does not have any NA and ay outlier, but its time series is corrupted, therefore we need valid windows\n",
    "def extract_valid_windows(df, date_col, input_window, target_window, input_columns, target_columns):\n",
    "    \"\"\"\n",
    "    Extracts valid windows from a time series DataFrame for LSTM training.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The time series DataFrame with a datetime column.\n",
    "        date_col (str): The name of the datetime column.\n",
    "        input_window (int): The number of timesteps for the input sequence.\n",
    "        target_window (int): The number of timesteps for the target sequence.\n",
    "        input_columns (list of str): List of column names to include in the input data.\n",
    "        target_columns (list of str): List of column names to include in the target data.\n",
    "        \n",
    "    Returns:\n",
    "        inputs (list of np.ndarray): List of valid input sequences.\n",
    "        targets (list of np.ndarray): List of corresponding target sequences.\n",
    "    \"\"\"\n",
    "    # Sort by the datetime column to ensure the time series is ordered\n",
    "    df = df.sort_values(by=date_col).reset_index(drop=True)\n",
    "    \n",
    "    # Ensure the datetime column is in pandas datetime format\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    \n",
    "    # Identify valid consecutive rows (1-hour apart)\n",
    "    time_diffs = df[date_col].diff().dt.total_seconds()\n",
    "    valid_indices = time_diffs == 3600  # 1 hour = 3600 seconds\n",
    "    \n",
    "    # Mark valid sequences\n",
    "    valid_sequence_flags = valid_indices | valid_indices.shift(-1, fill_value=False)\n",
    "    df = df[valid_sequence_flags].reset_index(drop=True)\n",
    "\n",
    "    # Prepare inputs and targets\n",
    "    inputs, targets = [], []\n",
    "    total_window = input_window + target_window\n",
    "\n",
    "    for i in range(len(df) - total_window + 1):\n",
    "        # Extract a potential window of size `total_window`\n",
    "        window = df.iloc[i:i+total_window]\n",
    "        \n",
    "        # Check if all rows in the window are 1-hour apart\n",
    "        if (window[date_col].diff().dt.total_seconds()[1:] == 3600).all():\n",
    "            # Split into input and target based on specified columns\n",
    "            input_data = window.iloc[:input_window][input_columns].values\n",
    "            target_data = window.iloc[input_window:][target_columns].values\n",
    "            inputs.append(input_data)\n",
    "            targets.append(target_data)\n",
    "\n",
    "    return np.array(inputs), np.array(targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df = pd.read_csv('merged_crypto_data.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/ds/cbsqg3fj1916q_s23ss9nk7w0000gn/T/ipykernel_12812/3944585468.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n"
     ]
    }
   ],
   "source": [
    "# Drop the second occurrence of a specific column\n",
    "merged_df = merged_df.loc[:, ~merged_df.columns.duplicated()]\n",
    "# data without NA rows if we needed \n",
    "filtered_df = merged_df[(merged_df['borrowing_rate'] != -50)&(merged_df['lending_rate'] != -50)&(merged_df['utilization_rate'] != -50)]\n",
    "filtered_df.reset_index(inplace=True, drop=True)\n",
    "# date formatting\n",
    "filtered_df['date'] = pd.to_datetime(filtered_df['date'])\n",
    "# taking the columns we want and converting them to floats\n",
    "filtered_df = filtered_df[['crypto_symbol', 'date', 'lending_rate',\t'borrowing_rate','utilization_rate','close', 'volume']]\n",
    "filtered_df[['lending_rate',\t'borrowing_rate','utilization_rate','close', 'volume']] = filtered_df[['lending_rate','borrowing_rate','utilization_rate','close', 'volume']].astype(float)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BATUSDT 28159\n",
      "LINKUSDT 31934\n",
      "KNCUSDT 27690\n",
      "MKRUSDT 30094\n",
      "MANAUSDT 27145\n",
      "ZRXUSDT 28255\n",
      "SNXUSDT 31087\n",
      "WBTCUSDT 11729\n",
      "ENJUSDT 27034\n",
      "RENUSDT 27658\n",
      "YFIUSDT 29900\n",
      "UNIUSDT 29885\n",
      "CRVUSDT 28455\n",
      "BALUSDT 24855\n",
      "ENSUSDT 20745\n",
      "1INCHUSDT 14025\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Initialize empty lists to store aggregated inputs and targets\n",
    "all_inputs = []\n",
    "all_targets = []\n",
    "\n",
    "# looping through symbols\n",
    "symbols = filtered_df['crypto_symbol'].unique()\n",
    "for s in symbols:\n",
    "     sim_df = filtered_df[filtered_df['crypto_symbol'] == s]\n",
    "     sim_df.reset_index(inplace=True, drop=True)\n",
    "     print(s, len(sim_df))\n",
    "     # First Loop: Calculate intervals for each column without modifying the DataFrame\n",
    "     intervals = {}\n",
    "     for column in ['lending_rate', 'borrowing_rate', 'utilization_rate']:\n",
    "          lower, upper = calculate_iqr_bounds(sim_df[column])\n",
    "          intervals[column] = {'lower_bound': lower, 'upper_bound': upper}\n",
    "\n",
    "     # Second Loop: Filter rows based on the pre-calculated intervals\n",
    "     reduced_df = sim_df.copy()\n",
    "     for column in ['lending_rate', 'borrowing_rate', 'utilization_rate']:\n",
    "          lower_bound = intervals[column]['lower_bound']\n",
    "          upper_bound = intervals[column]['upper_bound']\n",
    "          # Apply filtering based on pre-calculated bounds\n",
    "          reduced_df = reduced_df[(reduced_df[column] > lower_bound) & (reduced_df[column] < upper_bound)]\n",
    "\n",
    "     reduced_df['returns'] = calculate_hourly_returns(reduced_df, 'date', 'close')\n",
    "\n",
    "     # MinMax scaling\n",
    "     scaler = MinMaxScaler(feature_range=(0, 1))  # Default range is (0, 1)\n",
    "\n",
    "     scaled_df = reduced_df.copy()\n",
    "     scaled_df['lending_rate'] = scaler.fit_transform(reduced_df[['lending_rate']])\n",
    "     scaled_df['borrowing_rate'] = scaler.fit_transform(reduced_df[['borrowing_rate']])\n",
    "     scaled_df['utilization_rate'] = scaler.fit_transform(reduced_df[['utilization_rate']])\n",
    "     scaled_df['close'] = scaler.fit_transform(reduced_df[['close']])\n",
    "     scaled_df['volume'] = scaler.fit_transform(reduced_df[['volume']])\n",
    "     scaled_df['returns'] = scaler.fit_transform(reduced_df[['returns']])\n",
    "\n",
    "     inputs, targets = extract_valid_windows(\n",
    "          scaled_df,\n",
    "          'date', \n",
    "          40, 10, \n",
    "          ['lending_rate',\t'borrowing_rate','utilization_rate','returns', 'volume'], \n",
    "          ['lending_rate','borrowing_rate']\n",
    "          )\n",
    "     \n",
    "     # Append results from the current DataFrame\n",
    "     all_inputs.append(inputs)\n",
    "     all_targets.append(targets)\n",
    "     \n",
    "\n",
    "# Concatenate all inputs and targets into single arrays\n",
    "all_inputs = np.concatenate(all_inputs, axis=0) if all_inputs else np.array([])\n",
    "all_targets = np.concatenate(all_targets, axis=0) if all_targets else np.array([])\n",
    "     \n",
    "          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(111053, 40, 5)\n",
      "(111053, 10, 2)\n"
     ]
    }
   ],
   "source": [
    "print(all_inputs.shape)\n",
    "print(all_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nasimzarei/anaconda3/lib/python3.11/site-packages/keras/src/layers/rnn/rnn.py:204: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling LSTMCell.call().\n\n\u001b[1mDimensions must be equal, but are 5 and 6 for '{{node sequential_5_1/lstm_10_1/lstm_cell_1/MatMul}} = MatMul[T=DT_FLOAT, grad_a=false, grad_b=false, transpose_a=false, transpose_b=false](sequential_5_1/lstm_10_1/strided_slice_2, sequential_5_1/lstm_10_1/lstm_cell_1/Cast/ReadVariableOp)' with input shapes: [?,5], [6,256].\u001b[0m\n\nArguments received by LSTMCell.call():\n  • inputs=tf.Tensor(shape=(None, 5), dtype=float32)\n  • states=('tf.Tensor(shape=(None, 64), dtype=float32)', 'tf.Tensor(shape=(None, 64), dtype=float32)')\n  • training=True",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[58], line 30\u001b[0m\n\u001b[1;32m     26\u001b[0m lstm_model \u001b[38;5;241m=\u001b[39m create_lstm_model(input_shape\u001b[38;5;241m=\u001b[39m(input_window, \u001b[38;5;28mlen\u001b[39m(input_columns)), \n\u001b[1;32m     27\u001b[0m                                output_shape\u001b[38;5;241m=\u001b[39mtarget_window \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mlen\u001b[39m(target_columns))\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m history \u001b[38;5;241m=\u001b[39m lstm_model\u001b[38;5;241m.\u001b[39mfit(\n\u001b[1;32m     31\u001b[0m     X_train,\n\u001b[1;32m     32\u001b[0m     y_train\u001b[38;5;241m.\u001b[39mreshape(y_train\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m),  \n\u001b[1;32m     33\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     34\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[1;32m     35\u001b[0m     validation_split\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m  \u001b[38;5;66;03m# Automatically splits 20% of X_train and y_train for validation\u001b[39;00m\n\u001b[1;32m     36\u001b[0m )\n\u001b[1;32m     39\u001b[0m test_loss, test_mae \u001b[38;5;241m=\u001b[39m lstm_model\u001b[38;5;241m.\u001b[39mevaluate(X_test, y_test\u001b[38;5;241m.\u001b[39mreshape(y_test\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTest Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Test MAE: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtest_mae\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/keras/src/utils/traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[0;31mValueError\u001b[0m: Exception encountered when calling LSTMCell.call().\n\n\u001b[1mDimensions must be equal, but are 5 and 6 for '{{node sequential_5_1/lstm_10_1/lstm_cell_1/MatMul}} = MatMul[T=DT_FLOAT, grad_a=false, grad_b=false, transpose_a=false, transpose_b=false](sequential_5_1/lstm_10_1/strided_slice_2, sequential_5_1/lstm_10_1/lstm_cell_1/Cast/ReadVariableOp)' with input shapes: [?,5], [6,256].\u001b[0m\n\nArguments received by LSTMCell.call():\n  • inputs=tf.Tensor(shape=(None, 5), dtype=float32)\n  • states=('tf.Tensor(shape=(None, 64), dtype=float32)', 'tf.Tensor(shape=(None, 64), dtype=float32)')\n  • training=True"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "input_window = 40\n",
    "target_window = 5\n",
    "\n",
    "input_columns = ['high', 'low', 'close', 'volume', 'utilization_rate', 'stable_borrow_rate']\n",
    "\n",
    "target_columns = ['borrowing_rate','lending_rate' ]\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_inputs, all_targets, test_size=0.2, random_state=12)\n",
    "\n",
    "\n",
    "def create_lstm_model(input_shape, output_shape, dropout_rate=0.2):\n",
    "    \"\"\"Creates an LSTM model for time-series prediction.\"\"\"\n",
    "    model = Sequential([\n",
    "        LSTM(64, activation='tanh', input_shape=(input_window, len(input_columns)), return_sequences=True),\n",
    "        Dropout(dropout_rate),\n",
    "        LSTM(32, activation='tanh', return_sequences=False),  # Return the last hidden state\n",
    "        Dense(output_shape) \n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mse', metrics=['mae'])\n",
    "    return model\n",
    "\n",
    "lstm_model = create_lstm_model(input_shape=(input_window, len(input_columns)), \n",
    "                               output_shape=target_window * len(target_columns))\n",
    "\n",
    "# Train the model\n",
    "history = lstm_model.fit(\n",
    "    X_train,\n",
    "    y_train.reshape(y_train.shape[0], -1),  \n",
    "    epochs=2,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2  # Automatically splits 20% of X_train and y_train for validation\n",
    ")\n",
    "\n",
    "\n",
    "test_loss, test_mae = lstm_model.evaluate(X_test, y_test.reshape(y_test.shape[0], -1))\n",
    "print(f\"Test Loss: {test_loss}, Test MAE: {test_mae}\")\n",
    "\n",
    "predictions = lstm_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
