{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Masking, Reshape, Layer, Lambda, Concatenate, LayerNormalization, MultiHeadAttention, Add, Flatten, Dot\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "from keras.layers import BatchNormalization\n",
    "from tensorflow.keras.initializers import HeNormal\n",
    "\n",
    "def grn(input,hidden_size=None):\n",
    "    if hidden_size == None:\n",
    "        first_dense = Dense(units=input.shape[-1],activation=\"elu\")(input)\n",
    "    else:\n",
    "        first_dense = Dense(hidden_size, activation=\"elu\")(input)\n",
    "    dense_gate = Dense(units=first_dense.shape[-1], activation=\"sigmoid\")(first_dense)\n",
    "    skip_conn = Add()[input, dense_gate]\n",
    "    norm = LayerNormalization()(skip_conn)\n",
    "    return norm\n",
    "\n",
    "def variable_selection(input,hidden_size):\n",
    "    grn_1 = grn(input=input,hidden_size=hidden_size)\n",
    "    flatten_ = Flatten()(input)\n",
    "    grn_2 = grn(input=flatten_,hidden_size=hidden_size)\n",
    "    dense_softmax = Dense(unites=grn_2.shape[-1], activation=\"softmax\")(grn_2)\n",
    "    combined = Dot(axes=-1)([grn_1,dense_softmax])\n",
    "    return combined\n",
    "    \n",
    "def TFT(X_train_static,X_valid_static,X_test_static,X_train_past,X_valid_past,X_test_past,\n",
    "        X_train_future,X_valid_future,X_test_future,\n",
    "        Y_train,Y_valid,Y_test,weights, epochs=10,\n",
    "                     batch_size=100, hidden_size = 80, attention_heads=4):\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    n_classes = 3\n",
    "\n",
    "    inputs_static = Input(shape=(X_train_static.shape[1], X_train_static.shape[2]))\n",
    "    inputs_past = Input(shape=(X_train_past.shape[1], X_train_past.shape[2])) \n",
    "    inputs_future = Input(shape=(X_train_future.shape[1], X_train_future.shape[2]))\n",
    "\n",
    "    variable_selection_static = variable_selection(inputs_static, hidden_size=hidden_size)\n",
    "    variable_selection_past = variable_selection(inputs_past, hidden_size=hidden_size)\n",
    "    variable_selection_future = variable_selection(inputs_future, hidden_size=hidden_size)\n",
    "\n",
    "    lstm_past = LSTM(hidden_size)(variable_selection_past)\n",
    "    lstm_future = LSTM(hidden_size)(variable_selection_future)\n",
    "\n",
    "    lstm_gate_past = Dense(hidden_size,activation=\"sigmoid\")(lstm_past)\n",
    "    lstm_gate_future = Dense(hidden_size,activation=\"sigmoid\")(lstm_future)\n",
    "\n",
    "    skip_lstm_past = Add()([variable_selection_past,lstm_gate_past])\n",
    "    skip_lstm_future = Add()([variable_selection_future,lstm_gate_future])\n",
    "\n",
    "    norm_lstm_past = LayerNormalization()(skip_lstm_past)\n",
    "    nomr_lstm_future = LayerNormalization()(skip_lstm_future)\n",
    "\n",
    "    combine_past_static = Concatenate(axis=-1)([norm_lstm_past,variable_selection_static])\n",
    "    combine_future_static = Concatenate(axis=-1)([nomr_lstm_future,variable_selection_static])\n",
    "\n",
    "    grn_past = grn(combine_past_static,hidden_size=hidden_size)\n",
    "    grn_future = grn(combine_future_static,hidden_size=hidden_size)\n",
    "\n",
    "    attention_past = MultiHeadAttention(\n",
    "    num_heads=attention_heads,\n",
    "    key_dim=hidden_size//attention_heads)(\n",
    "    query=grn_past,\n",
    "    key=grn_past,\n",
    "    value=grn_past)\n",
    "    residual_1 = Add()([grn_past, attention_past])  # Add residual connection\n",
    "    norm_1 = LayerNormalization()(residual_1)\n",
    "\n",
    "    attention_future = MultiHeadAttention(\n",
    "    num_heads=attention_heads,\n",
    "    key_dim=hidden_size//attention_heads)(\n",
    "    query=grn_future,\n",
    "    key=grn_future,\n",
    "    value=grn_future)\n",
    "    residual_2 = Add()([grn_future, attention_future])\n",
    "    norm_2 = LayerNormalization()(residual_2)\n",
    "\n",
    "    cross_attention = MultiHeadAttention(\n",
    "    num_heads=attention_heads,\n",
    "    key_dim=hidden_size//attention_heads)(\n",
    "    # Query comes from the future sequence\n",
    "    query=norm_2,\n",
    "    # Keys and values come from the past sequence\n",
    "    key=norm_1,\n",
    "    value=norm_1)\n",
    "\n",
    "    # Add residual connection with the future sequence\n",
    "    residual_cross = Add()([norm_2, cross_attention])\n",
    "\n",
    "    # Apply layer normalization\n",
    "    norm_cross = LayerNormalization()(residual_cross)\n",
    "\n",
    "    grn_cross = grn(norm_cross,hidden_size=hidden_size)\n",
    "\n",
    "    class_predictions = Dense(n_classes, activation='softmax',name=\"class\", kernel_initializer=HeNormal())(grn_cross)\n",
    "    Attention_base = Model(inputs=[X_train_static,X_train_past,X_train_future], outputs=class_predictions)\n",
    "\n",
    "    # we will keep this as a standardized learning rate optimizer across all models\n",
    "    optimizer = Adadelta(\n",
    "    learning_rate=1.0,\n",
    "    rho=0.8,\n",
    "    epsilon=1e-7)      # Default , to prevent division by zero)\n",
    "\n",
    "    Attention_base.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['f1_score'])\n",
    "\n",
    "    Attention_base.summary()\n",
    "\n",
    "    # Pass the callback during training\n",
    "    history = Attention_base.fit(\n",
    "    x=[X_train_static,X_train_past,X_train_future], y=Y_train,\n",
    "    validation_data=([X_valid_static,X_valid_past,X_valid_future], Y_valid),\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    class_weight=weights,\n",
    "    shuffle=False)\n",
    "\n",
    "    fig, ax1 = plt.subplots()\n",
    "\n",
    "    # Plot losses on the primary y-axis\n",
    "    ax1.set_xlabel('Epoch')\n",
    "    ax1.set_ylabel('Loss', color='tab:red')\n",
    "    ax1.plot(history.history['loss'], label='Train Loss', color='red', linestyle='-')\n",
    "    ax1.plot(history.history['val_loss'], label='Validation Loss', color='red', linestyle='--')\n",
    "    ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "    # Create a second y-axis for accuracy\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.set_ylabel('Accuracy', color='tab:blue')\n",
    "    ax2.plot(history.history['f1_score'], label='Train f1_score', color='blue', linestyle='-')\n",
    "    ax2.plot(history.history['val_f1_score'], label='Validation f1_score', color='blue', linestyle='--')\n",
    "    ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "    # Combine legends from both axes\n",
    "    fig.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2)  # Legend outside the plot\n",
    "\n",
    "    plt.title('Model Accuracy and Loss')\n",
    "    plt.tight_layout()  # Adjust layout to avoid clipping\n",
    "    plt.show()\n",
    "\n",
    "    y_pred = Attention_base.predict(X_test_static,X_test_past,X_test_future)\n",
    "    y_pred = np.argmax(y_pred, axis=-1)\n",
    "\n",
    "    return Y_test, y_pred"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
