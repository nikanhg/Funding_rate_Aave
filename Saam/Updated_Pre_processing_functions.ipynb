{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated Extract Windows for the pre processing file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_valid_windows_v4(df, date_col, input_window, target_window, input_columns, target_columns,  train_end_date, valid_end_date):\n",
    "    \"\"\"\n",
    "    Extracts valid windows from a time series DataFrame for LSTM training.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The time series DataFrame with a datetime column.\n",
    "        date_col (str): The name of the datetime column.\n",
    "        input_window (int): The number of timesteps for the input sequence.\n",
    "        target_window (int): The number of timesteps for the target sequence.\n",
    "        input_columns (list of str): List of column names to include in the input data.\n",
    "        target_columns (list of str): List of column names to include in the target data.\n",
    "        \n",
    "    Returns:\n",
    "        inputs (list of np.ndarray): List of valid input sequences.\n",
    "        targets (list of np.ndarray): two values\n",
    "    \"\"\"\n",
    "    # Sort by the datetime column to ensure the time series is ordered\n",
    "    df = df.sort_values(by=date_col).reset_index(drop=True)\n",
    "\n",
    "    train_end_date = pd.to_datetime(train_end_date)\n",
    "    valid_end_date = pd.to_datetime(valid_end_date)\n",
    "    \n",
    "    # Ensure the datetime column is in pandas datetime format\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    \n",
    "    # Identify valid consecutive rows (1-hour apart)\n",
    "    time_diffs = df[date_col].diff().dt.total_seconds()\n",
    "    valid_indices = time_diffs == 3600  # 1 hour = 3600 seconds\n",
    "    \n",
    "    # Mark valid sequences\n",
    "    valid_sequence_flags = valid_indices | valid_indices.shift(-1, fill_value=False)\n",
    "    df = df[valid_sequence_flags].reset_index(drop=True)\n",
    "\n",
    "    # Prepare inputs and targets\n",
    "    input_train = []\n",
    "    input_valid = []\n",
    "    input_test = []\n",
    "    target_train = []\n",
    "    target_valid = []\n",
    "    target_test = []\n",
    "\n",
    "\n",
    "    total_window = input_window + target_window\n",
    "\n",
    "    for i in range(len(df) - total_window + 1):\n",
    "        # Extract a potential window of size `total_window`\n",
    "        window = df.iloc[i:i+total_window]\n",
    "        window_end_date = window[date_col].iloc[-1]\n",
    "        \n",
    "        # Check if all rows in the window are 1-hour apart\n",
    "        if (window[date_col].diff().dt.total_seconds()[1:] == 3600).all():\n",
    "            # Split into input and target based on specified columns\n",
    "            input_data = window.iloc[:input_window][input_columns].values\n",
    "            target_data = window.iloc[input_window:][target_columns].values\n",
    "            \n",
    "            # Calculate differences and sign\n",
    "            differences = target_data[-1, :] - target_data[0, :]\n",
    "            differences = custom_sign(differences)\n",
    "            \n",
    "            # Categorize the window based on its end date\n",
    "            if window_end_date <= train_end_date:\n",
    "                input_train.append(input_data)\n",
    "                target_train.append(differences)\n",
    "            elif window_end_date <= valid_end_date:\n",
    "                input_valid.append(input_data)\n",
    "                target_valid.append(differences)\n",
    "            else:\n",
    "                input_test.append(input_data)\n",
    "                target_test.append(differences)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    inputs_train, inputs_valid, inputs_test = np.array(input_train), np.array(input_valid), np.array(input_test)\n",
    "    targets_train, targets_valid, targets_test = np.array(target_train), np.array(target_valid), np.array(target_test)\n",
    "\n",
    "    return inputs_train, inputs_valid, inputs_test, targets_train, targets_valid, targets_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Updated loop for the grid search file\n",
    "\n",
    "the loop to create windows is updated, I used dates to hard cut train, validation and test windows.\n",
    "Because of this some of the subseqeunt code also needs changes, including a slight change to the model function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize empty lists to store aggregated inputs and targets\n",
    "X_train,X_valid,X_test = [], [], []\n",
    "Y_train,Y_valid,Y_test = [], [], []\n",
    "\n",
    "# looping through symbols\n",
    "symbols = filtered_df['crypto_symbol'].unique()\n",
    "for s in symbols:\n",
    "     try:\n",
    "          sim_df = filtered_df[filtered_df['crypto_symbol'] == s]\n",
    "          sim_df.reset_index(inplace=True, drop=True)\n",
    "          print(s)\n",
    "          # First Loop: Calculate intervals for each column without modifying the DataFrame\n",
    "          intervals = {}\n",
    "          for column in ['lending_rate', 'borrowing_rate', 'utilization_rate']:\n",
    "               lower, upper = pre.calculate_iqr_bounds(sim_df[column],outlier_threshold)\n",
    "               intervals[column] = {'lower_bound': lower, 'upper_bound': upper}\n",
    "\n",
    "          # getting the returns\n",
    "          reduced_df = sim_df.copy()\n",
    "          reduced_df['returns'] = pre.calculate_hourly_returns(reduced_df, 'date', 'close')\n",
    "          reduced_df = reduced_df[reduced_df['returns'].notna()]\n",
    "          reduced_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "          # Second Loop: Filter rows based on the pre-calculated intervals\n",
    "          for column in ['lending_rate', 'borrowing_rate', 'utilization_rate']:\n",
    "               lower_bound = intervals[column]['lower_bound']\n",
    "               upper_bound = intervals[column]['upper_bound']\n",
    "               # Apply filtering based on pre-calculated bounds\n",
    "               reduced_df = reduced_df[(reduced_df[column] > lower_bound) & (reduced_df[column] < upper_bound)]\n",
    "\n",
    "          reduced_df.reset_index(inplace=True, drop=True)\n",
    "\n",
    "          # MinMax scaling\n",
    "          scaler = MinMaxScaler(feature_range=(0, 1))  # Default range is (0, 1)\n",
    "\n",
    "          scaled_df = reduced_df.copy()\n",
    "          scaled_df['lending_rate'] = scaler.fit_transform(reduced_df[['lending_rate']])\n",
    "          scaled_df['borrowing_rate'] = scaler.fit_transform(reduced_df[['borrowing_rate']])\n",
    "          scaled_df['utilization_rate'] = scaler.fit_transform(reduced_df[['utilization_rate']])\n",
    "          scaled_df['close'] = scaler.fit_transform(reduced_df[['close']])\n",
    "          scaled_df['volume'] = scaler.fit_transform(reduced_df[['volume']])\n",
    "          scaled_df['returns'] = scaler.fit_transform(reduced_df[['returns']])\n",
    "\n",
    "          inputs_train, inputs_valid, inputs_test, targets_train, targets_valid, targets_test = pre.extract_valid_windows_v4(\n",
    "               scaled_df,\n",
    "               'date', \n",
    "               input_window, output_window, \n",
    "               input_columns, \n",
    "               ['lending_rate','borrowing_rate']\n",
    "               ,train_end_date=\"2023-07-01 00:00:00\", valid_end_date=\"2023-09-25 00:00:00\")\n",
    "\n",
    "          # Append results from the current DataFrame\n",
    "\n",
    "          X_train.append(inputs_train)\n",
    "          X_valid.append(inputs_valid)\n",
    "          X_test.append(inputs_test)\n",
    "          Y_train.append(targets_train)\n",
    "          Y_valid.append(targets_valid)\n",
    "          Y_test.append(targets_test)\n",
    "\n",
    "\n",
    "     except Exception as e:\n",
    "        # Handle any other exceptions\n",
    "        print(f\"Unexpected error in symbol {s}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate all inputs and targets into single arrays\n",
    "X_train = [arr for arr in X_train if len(arr) > 0]\n",
    "X_valid = [arr for arr in X_valid if len(arr) > 0]\n",
    "X_test = [arr for arr in X_test if len(arr) > 0]\n",
    "\n",
    "Y_train = [arr for arr in Y_train if len(arr) > 0]\n",
    "Y_valid = [arr for arr in Y_valid if len(arr) > 0]\n",
    "Y_test = [arr for arr in Y_test if len(arr) > 0]\n",
    "\n",
    "X_train  = np.concatenate(X_train, axis=0) if X_train else np.array([])\n",
    "X_valid = np.concatenate(X_valid, axis=0) if X_valid else np.array([])\n",
    "X_test = np.concatenate(X_test, axis=0) if X_test else np.array([])\n",
    "\n",
    "Y_train  = np.concatenate(Y_train, axis=0) if Y_train else np.array([])\n",
    "Y_valid = np.concatenate(Y_valid, axis=0) if Y_valid else np.array([])\n",
    "Y_test  = np.concatenate(Y_test, axis=0) if Y_test else np.array([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lending rate\n",
    "lending_rate_Y_train = Y_train[:, 0].reshape(Y_train.shape[0], 1) \n",
    "lending_rate_Y_valid = Y_valid[:, 0].reshape(Y_valid.shape[0], 1) \n",
    "lending_rate_Y_test = Y_test[:, 0].reshape(Y_test.shape[0], 1) \n",
    "# Borrowing rate\n",
    "borrow_rate_Y_train = Y_train[:, 1].reshape(Y_train.shape[0], 1) \n",
    "borrow_rate_Y_valid = Y_valid[:, 1].reshape(Y_valid.shape[0], 1) \n",
    "borrow_rate_Y_test = Y_test[:, 1].reshape(Y_test.shape[0], 1) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Dropout, Masking, Reshape, Layer, Lambda, Concatenate, LayerNormalization, MultiHeadAttention\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from tensorflow.keras.optimizers import Adadelta\n",
    "from keras.layers import BatchNormalization\n",
    "\n",
    "def train_v1(X_train,X_valid,X_test,Y_train,Y_valid,Y_test, epochs=10, batch_size=32, d1=0.1, d2 = 0.05, cell_size = 80, details=True):\n",
    "    # Clearing the TensorFlow session to ensure the model starts with fresh weights and biases\n",
    "    tf.keras.backend.clear_session()\n",
    "    n_classes = 3\n",
    "\n",
    "    cell_size_1 = cell_size\n",
    "    cell_size_2 = cell_size_1//2\n",
    "    \n",
    "    # Model definition\n",
    "    inputs = Input(shape=(X_train.shape[1], X_train.shape[2]))\n",
    "    Lstm_layer_1 = LSTM(cell_size_1, return_sequences=True, stateful=False)(inputs)\n",
    "    Batch_norm_1 = BatchNormalization()(Lstm_layer_1)\n",
    "    Dropout_layer_1 = Dropout(d1)(Batch_norm_1)\n",
    "    Lstm_layer_2 = LSTM(cell_size_2, return_sequences=False, stateful=False)(Dropout_layer_1)  # just halved\n",
    "    Batch_norm_2 = BatchNormalization()(Lstm_layer_2)\n",
    "    Drouput_layer_2 = Dropout(d2)(Batch_norm_2)\n",
    "    predictions = Dense(n_classes, activation='softmax')(Drouput_layer_2)\n",
    "    LSTM_base = Model(inputs=inputs, outputs=predictions)\n",
    "\n",
    "    # optimizer\n",
    "    optimizer = Adadelta(\n",
    "    learning_rate=1.0,\n",
    "    rho=0.8,\n",
    "    epsilon=1e-7)      # Default , to prevent division by zero)\n",
    "\n",
    "    LSTM_base.compile(\n",
    "        optimizer=optimizer,\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy'])\n",
    "\n",
    "    # Training the model\n",
    "    history = LSTM_base.fit(x=X_train, y=Y_train,\n",
    "                    validation_data=(X_valid, Y_valid),\n",
    "                    epochs=epochs,\n",
    "                    batch_size=batch_size,\n",
    "                    shuffle=False)\n",
    " \n",
    "    if details == True:\n",
    "        LSTM_base.summary()\n",
    "        fig, ax1 = plt.subplots()\n",
    "\n",
    "        # Plot losses on the primary y-axis\n",
    "        ax1.set_xlabel('Epoch')\n",
    "        ax1.set_ylabel('Loss', color='tab:red')\n",
    "        ax1.plot(history.history['loss'], label='Train Loss', color='red', linestyle='-')\n",
    "        ax1.plot(history.history['val_loss'], label='Validation Loss', color='red', linestyle='--')\n",
    "        ax1.tick_params(axis='y', labelcolor='tab:red')\n",
    "\n",
    "        # Create a second y-axis for accuracy\n",
    "        ax2 = ax1.twinx()\n",
    "        ax2.set_ylabel('Accuracy', color='tab:blue')\n",
    "        ax2.plot(history.history['accuracy'], label='Train Accuracy', color='blue', linestyle='-')\n",
    "        ax2.plot(history.history['val_accuracy'], label='Validation Accuracy', color='blue', linestyle='--')\n",
    "        ax2.tick_params(axis='y', labelcolor='tab:blue')\n",
    "\n",
    "        # Combine legends from both axes\n",
    "        fig.legend(loc='upper center', bbox_to_anchor=(0.5, 1.15), ncol=2)  # Legend outside the plot\n",
    "\n",
    "        plt.title('Model Accuracy and Loss')\n",
    "        plt.tight_layout()  # Adjust layout to avoid clipping\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "    y_pred = LSTM_base.predict(X_test)\n",
    "    y_pred = np.argmax(y_pred, axis=-1)\n",
    "\n",
    "    return Y_test, y_pred\n",
    "\n",
    "# example model\n",
    "Y_test, y_pred = train_v1(X_train,X_valid,X_test,lending_rate_Y_train,lending_rate_Y_valid,lending_rate_Y_test)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
