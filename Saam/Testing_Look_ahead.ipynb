{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import mysql.connector\n",
    "from mysql.connector import Error\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "# MySQL database connection function\n",
    "def connect_to_database():\n",
    "    try:\n",
    "        # Establishing connection to the database\n",
    "        connection = mysql.connector.connect(\n",
    "            host='crypto-matter.c5eq66ogk1mf.eu-central-1.rds.amazonaws.com',\n",
    "            database='Crypto',\n",
    "            user='Jing',  # Replace with your actual first name\n",
    "            password='Crypto12!'\n",
    "        )\n",
    "\n",
    "        if connection.is_connected():\n",
    "            db_info = connection.get_server_info()\n",
    "            print(\"Connected to MySQL database, MySQL Server version: \", db_info)\n",
    "            return connection\n",
    "\n",
    "    except Error as e:\n",
    "        print(\"Error while connecting to MySQL\", e)\n",
    "        return None\n",
    "\n",
    "# Function to query merged data from crypto_lending_borrowing and crypto_price tables\n",
    "def query_merged_crypto_data(connection):\n",
    "    query = \"\"\"\n",
    "    SELECT clb.lending_rate, clb.borrowing_rate, clb.utilization_rate, clb.stable_borrow_rate,\n",
    "    cp.*, usb.yield\n",
    "    FROM crypto_lending_borrowing clb\n",
    "    JOIN crypto_price cp \n",
    "        ON clb.crypto_symbol = cp.crypto_symbol\n",
    "        AND clb.date = cp.date\n",
    "    LEFT JOIN US_Bond_Yield usb\n",
    "        ON clb.date = usb.date\n",
    "    WHERE UPPER(clb.crypto_symbol) IN ('1INCHUSDT', 'BALUSDT', 'BATUSDT', 'CRVUSDT', 'ENJUSDT', 'ENSUSDT', 'KNCUSDT', 'LINKUSDT', 'MANAUSDT', 'MKRUSDT', 'RENUSDT', 'SNXUSDT', 'UNIUSDT', 'WBTCUSDT', 'YFIUSDT', 'ZRXUSDT')\n",
    "    \"\"\"\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    try:\n",
    "        # Execute the query\n",
    "        cursor.execute(query)\n",
    "\n",
    "        # Fetch all results\n",
    "        results = cursor.fetchall()\n",
    "\n",
    "        # Get column names from cursor description\n",
    "        columns = [desc[0] for desc in cursor.description]\n",
    "\n",
    "        # Convert results to a Pandas DataFrame\n",
    "        df = pd.DataFrame(results, columns=columns)\n",
    "\n",
    "        return df\n",
    "\n",
    "    except Error as e:\n",
    "        print(f\"Error: {e}\")\n",
    "        return None\n",
    "    finally:\n",
    "        cursor.close()\n",
    "\n",
    "# Function to close the database connection\n",
    "def query_quit(connection):\n",
    "    if connection.is_connected():\n",
    "        connection.close()\n",
    "        print(\"MySQL connection is closed\")\n",
    "\n",
    "# Define a function to calculate outlier bounds using IQR\n",
    "def calculate_iqr_bounds(series, multiplier=1.5):\n",
    "    Q1 = series.quantile(0.25)\n",
    "    Q3 = series.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = max(Q1 - multiplier * IQR, 0)\n",
    "    upper_bound = Q3 + multiplier * IQR\n",
    "    return lower_bound, upper_bound\n",
    "\n",
    "# calculate returns on valid windows\n",
    "def calculate_hourly_returns(df, date_col, close_col):\n",
    "    \"\"\"\n",
    "    Calculates returns based on the close price, only if the date difference is 1 hour.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The DataFrame containing the time series data.\n",
    "        date_col (str): The name of the datetime column.\n",
    "        close_col (str): The name of the close price column.\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Series containing the calculated returns or None for invalid rows.\n",
    "    \"\"\"\n",
    "    # Ensure the date column is in datetime format\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    \n",
    "    # Sort by date to ensure sequential order\n",
    "    df = df.sort_values(by=date_col).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate the time difference between consecutive rows in hours\n",
    "    time_diff = df[date_col].diff().dt.total_seconds() / 3600\n",
    "    \n",
    "    # Calculate returns only for rows where time_diff == 1 hour\n",
    "    returns = np.where(\n",
    "        time_diff == 1,\n",
    "        (df[close_col] - df[close_col].shift(1)) / df[close_col].shift(1),\n",
    "        None\n",
    "    )\n",
    "    \n",
    "    return pd.Series(returns, index=df.index)\n",
    "\n",
    "# now we have a dataframe that does not have any NA and ay outlier, but its time series is corrupted, therefore we need valid windows\n",
    "def extract_valid_windows_v3(df, date_col, input_window, target_window, input_columns, target_columns,  train_end_date, valid_end_date):\n",
    "    \"\"\"\n",
    "    Extracts valid windows from a time series DataFrame for LSTM training.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The time series DataFrame with a datetime column.\n",
    "        date_col (str): The name of the datetime column.\n",
    "        input_window (int): The number of timesteps for the input sequence.\n",
    "        target_window (int): The number of timesteps for the target sequence.\n",
    "        input_columns (list of str): List of column names to include in the input data.\n",
    "        target_columns (list of str): List of column names to include in the target data.\n",
    "        \n",
    "    Returns:\n",
    "        inputs (list of np.ndarray): List of valid input sequences.\n",
    "        targets (list of np.ndarray): two values\n",
    "    \"\"\"\n",
    "    # Sort by the datetime column to ensure the time series is ordered\n",
    "    df = df.sort_values(by=date_col).reset_index(drop=True)\n",
    "\n",
    "    train_end_date = pd.to_datetime(train_end_date)\n",
    "    valid_end_date = pd.to_datetime(valid_end_date)\n",
    "    \n",
    "    # Ensure the datetime column is in pandas datetime format\n",
    "    df[date_col] = pd.to_datetime(df[date_col])\n",
    "    \n",
    "    # Identify valid consecutive rows (1-hour apart)\n",
    "    time_diffs = df[date_col].diff().dt.total_seconds()\n",
    "    valid_indices = time_diffs == 3600  # 1 hour = 3600 seconds\n",
    "    \n",
    "    # Mark valid sequences\n",
    "    valid_sequence_flags = valid_indices | valid_indices.shift(-1, fill_value=False)\n",
    "    df = df[valid_sequence_flags].reset_index(drop=True)\n",
    "\n",
    "    # Prepare inputs and targets\n",
    "    input_train = []\n",
    "    input_valid = []\n",
    "    input_test = []\n",
    "    target_train = []\n",
    "    target_valid = []\n",
    "    target_test = []\n",
    "\n",
    "\n",
    "    total_window = input_window + target_window\n",
    "\n",
    "    for i in range(len(df) - total_window + 1):\n",
    "        # Extract a potential window of size `total_window`\n",
    "        window = df.iloc[i:i+total_window]\n",
    "        window_end_date = window[date_col].iloc[-1]\n",
    "        \n",
    "        # Check if all rows in the window are 1-hour apart\n",
    "        if (window[date_col].diff().dt.total_seconds()[1:] == 3600).all():\n",
    "            # Split into input and target based on specified columns\n",
    "            input_data = window.iloc[:input_window][input_columns].values\n",
    "            target_data = window.iloc[input_window:][target_columns].values\n",
    "            \n",
    "            # Calculate differences and sign\n",
    "            differences = target_data[-1, :] - target_data[0, :]\n",
    "            differences = custom_sign(differences)\n",
    "            \n",
    "            # Categorize the window based on its end date\n",
    "            if window_end_date <= train_end_date:\n",
    "                input_train.append(input_data)\n",
    "                target_train.append(differences)\n",
    "            elif window_end_date <= valid_end_date:\n",
    "                input_valid.append(input_data)\n",
    "                target_valid.append(differences)\n",
    "            else:\n",
    "                input_test.append(input_data)\n",
    "                target_test.append(differences)\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    inputs_train, inputs_valid, inputs_test = np.array(input_train), np.array(input_valid), np.array(input_test)\n",
    "    targets_train, targets_valid, targets_test = np.array(target_train), np.array(target_valid), np.array(target_test)\n",
    "\n",
    "    return inputs_train, inputs_valid, inputs_test, targets_train, targets_valid, targets_test\n",
    "\n",
    "# Custom sign function\n",
    "def custom_sign(x):\n",
    "    return np.where(x > 0, 1, np.where(x == 0, 0, 2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_llm_embeddings(dataframe, col, n_components=10):\n",
    "    model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "    unique_values = dataframe[col].unique()\n",
    "    \n",
    "    # Get embeddings for the unique values\n",
    "    embeddings = model.encode(unique_values, show_progress_bar=False)\n",
    "\n",
    "    # Apply PCA to reduce dimensionality\n",
    "    pca = PCA(n_components=n_components)\n",
    "    reduced_embeddings = pca.fit_transform(embeddings)\n",
    "\n",
    "    # Create a DataFrame to hold the reduced embeddings\n",
    "    reduced_embeddings_df = pd.DataFrame(reduced_embeddings, columns=[f'{col}_embedding_{i+1}' for i in range(n_components)])\n",
    "\n",
    "    reduced_embeddings_df[col] = unique_values\n",
    "\n",
    "    dataframe = dataframe.merge(reduced_embeddings_df, on=col, how='left')\n",
    "\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to MySQL database, MySQL Server version:  8.0.39\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\sghia\\anaconda3\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       lending_rate borrowing_rate utilization_rate stable_borrow_rate  \\\n",
      "66769      0.000091       0.016295        -0.099061           0.038281   \n",
      "381288     0.000001       0.201231        -0.137197           0.009337   \n",
      "280447     0.000013       0.373045         0.201463           0.082865   \n",
      "200594     0.000000       0.201408        -0.117472           0.033946   \n",
      "414312     0.000000       0.200220         0.762957           0.031154   \n",
      "\n",
      "       crypto_symbol                date   high    low  close adj_close  \\\n",
      "66769       LINKUSDT 2024-10-04 10:00:00  11.05  10.96  10.99     10.99   \n",
      "381288       UNIUSDT 2024-10-04 10:00:00   6.71   6.67   6.67      6.67   \n",
      "280447       ENJUSDT 2024-10-04 10:00:00   0.15   0.15   0.15      0.15   \n",
      "200594       ZRXUSDT 2024-10-04 10:00:00   0.31   0.31   0.31      0.31   \n",
      "414312       CRVUSDT 2024-10-04 10:00:00   0.26   0.26   0.26      0.26   \n",
      "\n",
      "         volume market_cap coin_supply         yield  \\\n",
      "66769     78854       None        None  0.0000695858   \n",
      "381288    27137       None        None  0.0000695858   \n",
      "280447   255096       None        None  0.0000695858   \n",
      "200594   611831       None        None  0.0000695858   \n",
      "414312  1961620       None        None  0.0000695858   \n",
      "\n",
      "        crypto_symbol_embedding_1  \n",
      "66769                   -0.243431  \n",
      "381288                  -0.061505  \n",
      "280447                   0.754841  \n",
      "200594                  -0.244390  \n",
      "414312                   0.053779  \n"
     ]
    }
   ],
   "source": [
    "connect = connect_to_database()\n",
    "\n",
    "merged_df = query_merged_crypto_data(connect)\n",
    "\n",
    "merged_df_emb = create_llm_embeddings(merged_df, \"crypto_symbol\", n_components=1)\n",
    "\n",
    "merged_df_emb.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66769    2024-10-04 10:00:00\n",
      "381288   2024-10-04 10:00:00\n",
      "280447   2024-10-04 10:00:00\n",
      "200594   2024-10-04 10:00:00\n",
      "414312   2024-10-04 10:00:00\n",
      "Name: date, dtype: datetime64[ns]\n"
     ]
    }
   ],
   "source": [
    "merged_df_emb['date'] = pd.to_datetime(merged_df_emb['date'])\n",
    "\n",
    "# Sort in descending order\n",
    "merged_df_emb = merged_df_emb.sort_values(by='date', ascending=False)\n",
    "\n",
    "# Print the top rows after sorting\n",
    "print(merged_df_emb['date'].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "symbols = merged_df_emb[\"crypto_symbol\"].unique()\n",
    "X_train,X_valid,X_test = []\n",
    "Y_train,Y_valid,Y_test = []\n",
    "loop = 0\n",
    "for s in symbols:\n",
    "     loop = loop + 1\n",
    "     print(loop)\n",
    "     sim_df = merged_df_emb[merged_df_emb['crypto_symbol'] == s]\n",
    "     \n",
    "     inputs_train, inputs_valid, inputs_test, targets_train, targets_valid, targets_test = extract_valid_windows_v3(sim_df, 'date', 48,48, [\"date\", \"crypto_symbol_embedding_1\",\"lending_rate\"], [\"date\", \"crypto_symbol_embedding_1\",\"lending_rate\"], train_end_date=\"2024-01-01 00:00:00\", valid_end_date=\"2024-07-01 00:00:00\")\n",
    "     all_inputs.append(inputs)\n",
    "     all_targets.append(targets)\n",
    "\n",
    "\n",
    "X_train  = np.concatenate(X_train, axis=0) if X_train else np.array([])\n",
    "X_valid = np.concatenate(X_valid, axis=0) if X_valid else np.array([])\n",
    "X_test = np.concatenate(X_test, axis=0) if X_test else np.array([])\n",
    "\n",
    "Y_train  = np.concatenate(Y_train, axis=0) if Y_train else np.array([])\n",
    "Y_valid = np.concatenate(Y_valid, axis=0) if Y_valid else np.array([])\n",
    "Y_test  = np.concatenate(Y_test, axis=0) if Y_test else np.array([])\n",
    "\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(472515, 48, 3)\n",
      "(472515, 48, 3)\n"
     ]
    }
   ],
   "source": [
    "print(all_inputs.shape)\n",
    "print(all_targets.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Timestamp('2020-12-10 22:00:00') Timestamp('2020-12-10 23:00:00')\n",
      " Timestamp('2020-12-11 00:00:00') Timestamp('2020-12-11 01:00:00')\n",
      " Timestamp('2020-12-11 02:00:00') Timestamp('2020-12-11 03:00:00')\n",
      " Timestamp('2020-12-11 04:00:00') Timestamp('2020-12-11 05:00:00')\n",
      " Timestamp('2020-12-11 06:00:00') Timestamp('2020-12-11 07:00:00')\n",
      " Timestamp('2020-12-11 08:00:00') Timestamp('2020-12-11 09:00:00')\n",
      " Timestamp('2020-12-11 10:00:00') Timestamp('2020-12-11 11:00:00')\n",
      " Timestamp('2020-12-11 12:00:00') Timestamp('2020-12-11 13:00:00')\n",
      " Timestamp('2020-12-11 14:00:00') Timestamp('2020-12-11 15:00:00')\n",
      " Timestamp('2020-12-11 16:00:00') Timestamp('2020-12-11 17:00:00')\n",
      " Timestamp('2020-12-11 18:00:00') Timestamp('2020-12-11 19:00:00')\n",
      " Timestamp('2020-12-11 20:00:00') Timestamp('2020-12-11 21:00:00')\n",
      " Timestamp('2020-12-11 22:00:00') Timestamp('2020-12-11 23:00:00')\n",
      " Timestamp('2020-12-12 00:00:00') Timestamp('2020-12-12 01:00:00')\n",
      " Timestamp('2020-12-12 02:00:00') Timestamp('2020-12-12 03:00:00')\n",
      " Timestamp('2020-12-12 04:00:00') Timestamp('2020-12-12 05:00:00')\n",
      " Timestamp('2020-12-12 06:00:00') Timestamp('2020-12-12 07:00:00')\n",
      " Timestamp('2020-12-12 08:00:00') Timestamp('2020-12-12 09:00:00')\n",
      " Timestamp('2020-12-12 10:00:00') Timestamp('2020-12-12 11:00:00')\n",
      " Timestamp('2020-12-12 12:00:00') Timestamp('2020-12-12 13:00:00')\n",
      " Timestamp('2020-12-12 14:00:00') Timestamp('2020-12-12 15:00:00')\n",
      " Timestamp('2020-12-12 16:00:00') Timestamp('2020-12-12 17:00:00')\n",
      " Timestamp('2020-12-12 18:00:00') Timestamp('2020-12-12 19:00:00')\n",
      " Timestamp('2020-12-12 20:00:00') Timestamp('2020-12-12 21:00:00')]\n"
     ]
    }
   ],
   "source": [
    "print(all_inputs[200,:,0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Timestamp('2020-12-13 22:00:00') Timestamp('2020-12-13 23:00:00')\n",
      " Timestamp('2020-12-14 00:00:00') Timestamp('2020-12-14 01:00:00')\n",
      " Timestamp('2020-12-14 02:00:00') Timestamp('2020-12-14 03:00:00')\n",
      " Timestamp('2020-12-14 04:00:00') Timestamp('2020-12-14 05:00:00')\n",
      " Timestamp('2020-12-14 06:00:00') Timestamp('2020-12-14 07:00:00')\n",
      " Timestamp('2020-12-14 08:00:00') Timestamp('2020-12-14 09:00:00')\n",
      " Timestamp('2020-12-14 10:00:00') Timestamp('2020-12-14 11:00:00')\n",
      " Timestamp('2020-12-14 12:00:00') Timestamp('2020-12-14 13:00:00')\n",
      " Timestamp('2020-12-14 14:00:00') Timestamp('2020-12-14 15:00:00')\n",
      " Timestamp('2020-12-14 16:00:00') Timestamp('2020-12-14 17:00:00')\n",
      " Timestamp('2020-12-14 18:00:00') Timestamp('2020-12-14 19:00:00')\n",
      " Timestamp('2020-12-14 20:00:00') Timestamp('2020-12-14 21:00:00')\n",
      " Timestamp('2020-12-14 22:00:00') Timestamp('2020-12-14 23:00:00')\n",
      " Timestamp('2020-12-15 00:00:00') Timestamp('2020-12-15 01:00:00')\n",
      " Timestamp('2020-12-15 02:00:00') Timestamp('2020-12-15 03:00:00')\n",
      " Timestamp('2020-12-15 04:00:00') Timestamp('2020-12-15 05:00:00')\n",
      " Timestamp('2020-12-15 06:00:00') Timestamp('2020-12-15 07:00:00')\n",
      " Timestamp('2020-12-15 08:00:00') Timestamp('2020-12-15 09:00:00')\n",
      " Timestamp('2020-12-15 10:00:00') Timestamp('2020-12-15 11:00:00')\n",
      " Timestamp('2020-12-15 12:00:00') Timestamp('2020-12-15 13:00:00')\n",
      " Timestamp('2020-12-15 14:00:00') Timestamp('2020-12-15 15:00:00')\n",
      " Timestamp('2020-12-15 16:00:00') Timestamp('2020-12-15 17:00:00')\n",
      " Timestamp('2020-12-15 18:00:00') Timestamp('2020-12-15 19:00:00')\n",
      " Timestamp('2020-12-15 20:00:00') Timestamp('2020-12-15 21:00:00')]\n"
     ]
    }
   ],
   "source": [
    "print(all_targets[200,:,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
